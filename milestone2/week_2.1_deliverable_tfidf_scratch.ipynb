{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Pandemic prevention', 'ranking': 0.30402262201155605}, {'title': 'HIV/AIDS', 'ranking': 0.09659164974175692}, {'title': 'Event 201', 'ranking': 0.06555233534603662}, {'title': 'Pandemic Severity Assessment Framework', 'ranking': 0.05943509640611115}, {'title': 'Crimson Contagion', 'ranking': 0.046156414441719223}, {'title': 'HIV/AIDS in Yunnan', 'ranking': 0.04607708829458858}, {'title': 'Disease X', 'ranking': 0.03269277376102613}, {'title': 'Science diplomacy and pandemics', 'ranking': 0.026768805724627844}, {'title': 'Pandemic', 'ranking': 0.022854684339078644}, {'title': 'Swine influenza', 'ranking': 0.022307599487677646}, {'title': 'Spanish flu', 'ranking': 0.015809196450560063}, {'title': 'Pandemic severity index', 'ranking': 0.01265838869003447}, {'title': 'COVID-19 pandemic', 'ranking': 0.008168664115353613}, {'title': 'Plague of Cyprian', 'ranking': 0.006858360288458347}, {'title': 'PREDICT (USAID)', 'ranking': 0.0067443391786392365}, {'title': 'Antonine Plague', 'ranking': 0.003716411175404225}, {'title': '1929â€“1930 psittacosis pandemic', 'ranking': 0.0036378350274731894}, {'title': 'Epidemiology of HIV/AIDS', 'ranking': 0.002007447593205885}, {'title': 'Unified Victim Identification System', 'ranking': 0.0015014326501858165}, {'title': 'Cholera', 'ranking': 0.0014186715610547038}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def save_data(self, data ,save_path):\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        \n",
    "    def _lowercase(self, text):\n",
    "        return self.nlp(text.lower())\n",
    "\n",
    "\n",
    "    def _rm_stop_punct(self, text):\n",
    "        return [t for t in text if not t.is_punct and not t.is_stop]\n",
    "\n",
    "\n",
    "    def _lemmatizer(self, text):\n",
    "        return [t.lemma_ for t in text if t.dep_]\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        lower = self._lowercase(doc.text)\n",
    "        no_stop = self._rm_stop_punct(lower)\n",
    "        lemma = self._lemmatizer(no_stop)\n",
    "        return lemma\n",
    "\n",
    "\n",
    "class TFIDFVectFromScratch:\n",
    "\n",
    "    \"\"\"\n",
    "    This class is based on the solution provided for the liveproject milestone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_data):\n",
    "        self.data = text_data\n",
    "\n",
    "\n",
    "    def _build_flatten_vocab(self, text_var):\n",
    "        tokens = [token[text_var] for token in self.data]\n",
    "        return list(set([token for sub in tokens for token in sub]))\n",
    "\n",
    "\n",
    "    def _token_counter_within(self, text_var):\n",
    "        return [Counter(doc[text_var]) for doc in self.data]\n",
    "\n",
    "    \n",
    "    def _token_counter_across(self, token_counts, vocab):\n",
    "        return {token: sum([1 for doc in token_counts if token in doc ]) for token in vocab}\n",
    "\n",
    "    \n",
    "    def _generate_tfidf(self, tokens_within, vocabulary, term_text, tokens_across):\n",
    "        ### Function based on solution provided via LiveProject ###\n",
    "\n",
    "        # Iterate over tokens counted within docs\n",
    "        for idx, doc in enumerate(tokens_within):\n",
    "            tfidf_vector = []\n",
    "            # Iterate ober tokens counted across docs\n",
    "            for token in vocabulary:\n",
    "                # TF -> count per term per doc / doc length\n",
    "                tf = doc[token] / len(self.data[idx][term_text]) \n",
    "                # IDF -> num documents / num documents with term\n",
    "                idf = np.log(len(self.data) / tokens_across[token])\n",
    "                tfidf = tf * idf\n",
    "                tfidf_vector.append(tfidf)\n",
    "            self.data[idx]['tf_idf'] = tfidf_vector\n",
    "        return self.data\n",
    "  \n",
    "\n",
    "    def _generate_tfidf_query(self, vocabulary, tokens_across, q_tokenized):\n",
    "         ### Function based on solution provided via LiveProject ###\n",
    "\n",
    "        q_vector = TextProcessor().preprocess_text(q_tokenized)\n",
    "        q_counted = Counter(q_vector)\n",
    "        \n",
    "        q_vec = []\n",
    "        for doc in vocabulary:\n",
    "            tf = q_counted[doc] / len(q_tokenized)\n",
    "            idf = np.log(len(self.data) / tokens_across[doc])\n",
    "            tfidf = tf * idf\n",
    "            q_vec.append(tfidf)\n",
    "        return q_vec\n",
    "\n",
    "\n",
    "    def tfidf_generator(self, term_text, q_tokens, is_query=False):\n",
    "        vocab = self._build_flatten_vocab(term_text)\n",
    "        within = self._token_counter_within(term_text)\n",
    "        across = self._token_counter_across(within, vocab)\n",
    "        if not is_query:\n",
    "            return self._generate_tfidf(within, vocab, term_text, across)\n",
    "        else:\n",
    "            return self._generate_tfidf_query(vocab, across, q_tokens)\n",
    "\n",
    "\n",
    "class SimilaritySearch:\n",
    "\n",
    "    \"\"\"\n",
    "    This class is based on the solution provided for the liveproject milestone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_data):\n",
    "        self.data = text_data\n",
    "\n",
    "\n",
    "    def similarity_rankings(self, term_text, query_tfidf):\n",
    "        \"\"\" Function based on solution provided\"\"\"\n",
    "        q_vector = query_tfidf\n",
    "        q_array = np.array(q_vector)\n",
    "\n",
    "        doc_rankings = []\n",
    "        for doc in self.data:\n",
    "            ranking = {}\n",
    "            doc_array = np.array(doc[term_text])\n",
    "            similarity = cosine_similarity(q_array.reshape(1, -1), doc_array.reshape(1, -1))[0][0]\n",
    "            if similarity > 0:\n",
    "                ranking['title'] = doc['title']\n",
    "                ranking['ranking'] = similarity\n",
    "                doc_rankings.append(ranking)\n",
    "        return sorted(doc_rankings, key=lambda x: x['ranking'], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__== '__main__':\n",
    "    loader = DataLoader('../data_hub/processed_data.json')\n",
    "    data = loader.load_data()\n",
    "\n",
    "    tp = TextProcessor()\n",
    "    tf_vect = TFIDFVectFromScratch(data)\n",
    "    fs = SimilaritySearch(data)\n",
    "\n",
    "    query = \"pandemic prevention organizations\"\n",
    "\n",
    "    tfidf = tf_vect.tfidf_generator('tokenized_text', q_tokens=query)\n",
    "    q_tfidf = tf_vect.tfidf_generator('tokenized_text', q_tokens=query, is_query=True)\n",
    "    results = fs.similarity_rankings('tf_idf', q_tfidf)\n",
    "    print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92b3875b50552a19f5c8b904a9bd9455a012f7f4a6644b61f5845177f2c2dbee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('semantic_searchenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
