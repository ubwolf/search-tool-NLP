{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'black death', 'relevant_articles': [['Pandemic', 0.04679883867323402], ['Cholera', 0.014518164813892178], ['Antonine Plague', 0.013132843743864298], ['Epidemiology of HIV/AIDS', 0.011824072374200845], ['Bills of mortality', 0.008868054280650635], ['Spanish flu', 0.008559216569384194], ['1929â€“1930 psittacosis pandemic', 0.008115106275689732], ['Pandemic Severity Assessment Framework', 0.007964826529843625], ['HIV/AIDS', 0.006800010001763728], ['COVID-19 pandemic', 0.0050011701466459975], ['Swine influenza', 0.004551329445624929]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def save_data(self, data ,save_path):\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "       \n",
    "        \n",
    "    def _lowercase(self, text):\n",
    "        return self.nlp(text.lower())\n",
    "\n",
    "\n",
    "    def _rm_stop_punct(self, text):\n",
    "        return [t for t in text if not t.is_punct and not t.is_stop]\n",
    "\n",
    "\n",
    "    def _lemmatizer(self, text):\n",
    "        return [t.lemma_ for t in text if t.dep_]\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        lower = self._lowercase(doc.text)\n",
    "        no_stop = self._rm_stop_punct(lower)\n",
    "        lemma = self._lemmatizer(no_stop)\n",
    "        return lemma\n",
    "\n",
    "\n",
    "class TFIDFVectFromScratch:\n",
    "\n",
    "    \"\"\"\n",
    "    This class is based on the solution provided for the liveproject milestone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_data):\n",
    "        self.data = text_data\n",
    "    \n",
    "\n",
    "    def _build_flatten_vocab(self, text_var):\n",
    "        tokens = [token[text_var] for token in self.data]\n",
    "        return list(set([token for sub in tokens for token in sub]))\n",
    "\n",
    "\n",
    "    def _token_counter_within(self, text_var):\n",
    "        return [Counter(doc[text_var]) for doc in self.data]\n",
    "\n",
    "    \n",
    "    def _token_counter_across(self, token_counts, vocab):\n",
    "        return {token: sum([1 for doc in token_counts if token in doc ]) for token in vocab}\n",
    "\n",
    "    \n",
    "    def _generate_tfidf(self, tokens_within, vocabulary, term_text, tokens_across):\n",
    "         ### Function based on solution provided via LiveProject ###\n",
    "\n",
    "        # Iterate over tokens counted within docs\n",
    "        for idx, doc in enumerate(tokens_within):\n",
    "            tfidf_vector = []\n",
    "            # Iterate ober tokens counted across docs\n",
    "            for token in vocabulary:\n",
    "                # TF -> count per term per doc / doc length\n",
    "                tf = doc[token] / len(self.data[idx][term_text]) \n",
    "                # IDF -> num documents / num documents with term\n",
    "                idf = np.log(len(self.data) / tokens_across[token])\n",
    "                tfidf = tf * idf\n",
    "                tfidf_vector.append(tfidf)\n",
    "            self.data[idx]['tf_idf'] = tfidf_vector\n",
    "        return self.data\n",
    "  \n",
    "\n",
    "    def _generate_tfidf_query(self, vocabulary, tokens_across, q_tokenized):\n",
    "         ### Function based on solution provided via LiveProject ###\n",
    "\n",
    "        q_vector = TextProcessor().preprocess_text(q_tokenized)\n",
    "        q_counted = Counter(q_vector)\n",
    "        \n",
    "        q_vec = []\n",
    "        for doc in vocabulary:\n",
    "            tf = q_counted[doc] / len(q_tokenized)\n",
    "            idf = np.log(len(self.data) / tokens_across[doc])\n",
    "            tfidf = tf * idf\n",
    "            q_vec.append(tfidf)\n",
    "        return q_vec\n",
    "\n",
    "\n",
    "    def tfidf_generator(self, term_text, q_tokens, is_query=False):\n",
    "        vocab = self._build_flatten_vocab(term_text)\n",
    "        within = self._token_counter_within(term_text)\n",
    "        across = self._token_counter_across(within, vocab)\n",
    "        if not is_query:\n",
    "            return self._generate_tfidf(within, vocab, term_text, across)\n",
    "        else:\n",
    "            return self._generate_tfidf_query(vocab, across, q_tokens)\n",
    "\n",
    "\n",
    "class SimilaritySearch:\n",
    "\n",
    "    \"\"\"\n",
    "    This class is based on the solution provided for the liveproject milestone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_data):\n",
    "        self.data = text_data\n",
    "\n",
    "\n",
    "    def similarity_rankings(self, term_text, query_tfidf):\n",
    "        q_vector = query_tfidf\n",
    "        q_array = np.array(q_vector)\n",
    "\n",
    "        doc_rankings = []\n",
    "        for doc in self.data:\n",
    "            ranking = {}\n",
    "            doc_array = np.array(doc[term_text])\n",
    "            similarity = cosine_similarity(q_array.reshape(1, -1), doc_array.reshape(1, -1))[0][0]\n",
    "            if similarity > 0:\n",
    "                ranking['title'] = doc['title']\n",
    "                ranking['ranking'] = similarity\n",
    "                doc_rankings.append(ranking)\n",
    "        return sorted(doc_rankings, key=lambda x: x['ranking'], reverse=True)\n",
    "\n",
    "\n",
    "class InvertedIndexSearch:\n",
    "\n",
    "    def __init__(self, text_data, vocab):\n",
    "        self.data = text_data\n",
    "        self.vocabulary = vocab\n",
    "\n",
    "    \n",
    "    def _inverted_index(self):\n",
    "        inverted_idx = {}\n",
    "        for idx, word in enumerate(self.vocabulary):\n",
    "            inverted_idx[word] = []\n",
    "            for doc in self.data:\n",
    "                if word in doc['tokenized_text']:\n",
    "                    inverted_idx[word].append((doc['title'], doc['tf_idf'][idx]))\n",
    "        return inverted_idx\n",
    "\n",
    "\n",
    "    def search(self, query):\n",
    "        query_tokens = TextProcessor().preprocess_text(query)\n",
    "        inverted_idx = self._inverted_index()\n",
    "        # Generate index for query and flatten nested list\n",
    "        result_list = [x for i in [inverted_idx[token] for token in query_tokens] for x in i]\n",
    "        # Create tuple based on document title\n",
    "        titles = {x[0] for x in result_list}\n",
    "        # Add values if title occurs multiple times and sort\n",
    "        sums = sorted([(i, sum(x[1] for x in result_list if x[0] == i)) for i in titles], key=lambda x: x[1], reverse=True)\n",
    "        # Generate final output\n",
    "        return {'query': query, 'relevant_articles':[[x, y] for x, y in sums]}\n",
    "        \n",
    "\n",
    "\n",
    "if __name__== '__main__':\n",
    "    data_loader = DataLoader('../data_hub/processed_data_tfidf.json')\n",
    "    data = data_loader.load_data()\n",
    "\n",
    "    vocab_loader = DataLoader('../data_hub/vocab.json')\n",
    "    vocab = vocab_loader.load_data()\n",
    "\n",
    "    idx = InvertedIndexSearch(data, vocab)\n",
    "\n",
    "    query = 'black death'\n",
    "    results = idx.search(query) \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92b3875b50552a19f5c8b904a9bd9455a012f7f4a6644b61f5845177f2c2dbee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('semantic_searchenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
